Elasticsearch 
-----------------
what is indexing
Indexing means storing one or more documents in an index: a similar concept to inserting records in a relational database.




###Elastic configration

cluster.name
cluster.name: This sets up the name of the cluster. Only nodes with the same name can join together.

node.name: If not defined, this is automatically assigned by Elasticsearch.
network.host: defines the IP of your machine to bind the node. If your server is on different LANs

allows defining a name for the node. If you have a lot of nodes on different machines, it is useful to set their names to something meaningful in order to easily locate them. Using a valid name is easier to remember than a generated name such as fyBySLMcR3uqKiYC32P5Sg.

 Generally, a node name is the same as a host server name for easy maintenance.


discovery.zen.ping.unicast.hosts: allows you to define a list of hosts (with ports or a port range) to be used to discover other nodes to join the cluster. The preferred port is the transport one, usually 9300 

path.data: paramter is the most import one. This allows to define one more directories(in different disk)

path.work:parameter is a location in which Elasticsearch stores temporary files.

path.log: parameter is where log files are put

path.plugins:

index.number_of_shards: which controls the standard number of shards for a new created index.
index.number_of_replicas:  which controls the initial number of replicas.

index.auto_expand_replicas: This allows you to define a dynamic number of replicas related to the number of shards




####Security setting in elasticsearch 

To allow Elasticsearch to manage a large number of files, you need to increment the number of file descriptors (number of files) that a user can manage


/etc/security/limits.conf
elasticsearch - nofile 65536
elasticsearch - memlock unlimited


###To control memory swapping, you need to set up the following parameter in elasticsearch.yml
bootstrap.memory_lock: true

If you don't set bootstrap.memory_lock: true, Elasticsearch dumps the whole process memory on disk and defragments it back in memory, freezing the system. With this setting, the defragmentation step is done all in memory, with a huge performance boost.



##To fix the memory usage size of the Elasticsearch server, we need to set up the same values for Xms and Xmx
-Xms1g
-Xmx1g

The Xms and Xmx memory settings prevent Elasticsearch from swapping memory and give a performance boost in an environment. This setting is required because, during indexing and searching, Elasticsearch creates and destroys a lot of objects in memory

##node.master: true
Set up whether the node can be a master or not

#node.data: true
parameter allows you to store data in the node

#To have a High Availability (HA) cluster, you need at least three nodes that are masters with the value of minimum_master_nodes set to 2.




Designing a ES cluster
-----------------------------
How much data are you working with?
How many searches will you be processing?
How complex are your searches?
How much resources will each node have to work with?
How many indexes/applications will you be working with?

Default setting 
by default, the number of shards is 5 and the number of replicas is 1




Two ports required 
9300: This port is used for internal intranode communication.
9200: This port is used for the HTTP REST API.

If these port numbers are already bound, Elasticsearch automatically increments the port number and tries to bind on them until a port is available (that is, 9201, 9202, and so on





Setting up of elasticsearch logging 
------------------------------------
log4j2.properties
rootLogger.level = info
rootLogger.level = debug



file required for elasticsearch 
------------------------------------

Config files locationedit
Elasticsearch has three configuration files:

elasticsearch.yml for configuring Elasticsearch
jvm.options for configuring Elasticsearch JVM settings
log4j2.properties for configuring Elasticsearch logging




-----------------------------------
Installation procedure

##rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

##[root@node1 yum.repos.d]# cat > elasticsearch.repo
[elasticsearch-7.x]
name=Elasticsearch repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md


##systemctl start elasticserach

##[root@node1 yum.repos.d]# curl -X GET 'http://localhost:9200'
{
  "name" : "node1",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "rqcWtVKtSj-t68XbGiejsQ",
  "version" : {
    "number" : "7.2.0",
    "build_flavor" : "default",
    "build_type" : "rpm",
    "build_hash" : "508c38a",
    "build_date" : "2019-06-20T15:54:18.811730Z",
    "build_snapshot" : false,
    "lucene_version" : "8.0.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
[root@node1 yum.repos.d]#

Two


#Create a index
 curl -XPUT "http://localhost:9200/employee"

##if you are trying to create a index already exists
[root@node1 bin]# ./curl -XPUT localhost:9200/myindex
{"error":{"root_cause":[{"type":"resource_already_exists_exception","reason":"index [myindex/Gec-8BGLRwWvsSIwTc8iUw] already exists","index_uuid":"Gec-8BGLRwWvsSIwTc8iUw","index":"myindex"}],"type":"resource_already_exists_exception","reason":"index [myindex/Gec-8BGLRwWvsSIwTc8iUw] already exists","index_uuid":"Gec-8BGLRwWvsSIwTc8iUw","index":"myindex"},"status":400}[root@node1 bin]#


number_of_shards, which controls the number of shards that compose the index
each shard can hold two raised to 32 index whole.

#Delete a index
#delete a index 
./curl -XDELETE localhost:9200/myindex


#How the deletion works::
When an index is deleted, all the data related to the index is removed from the disk and is lost

The deleting process is composed of two steps: first, the cluster is updated, and then the shards are deleted from the storage. This operation is very quick; in a traditional filesystem, it is implemented as a recursive delete.	

In production, it is good practice to disable the all-indices deletion by adding the following line to elasticsearch.yml
action.destructive_requires_name:true

How GET request works
Internally, Elasticsearch spreads the get in parallel on several shards and collects the results to return to the user.



#TO list the index
[root@node1 opt]# curl -XGET http://localhost:9200/_cat/indices?v
health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   books    I8Zcx3VvR8aRrFDidOEqXg   1   1          0            0       230b           230b
yellow open   employee Dxb9NFNjQVCtKrm00Wx7ag   1   1          0            0       230b           230b




#Inserting data to elasticsearch
Download the data to elasticsearch
curl -O https://download.elastic.co/demos/kibana/gettingstarted/7.x/logs.jsonl.gz
gunzip logs.jsonl.gz
curl -H 'Content-Type: application/x-ndjson' -XPOST 'localhost:9200/_bulk?pretty' --data-binary @logs.jsonl



[root@node1 opt]# curl -XGET http://localhost:9200/_cat/indices?v
health status index               uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   logstash-2015.05.19 JIohpLNbRGmQkobr67liTw   1   1       4624            0     17.7mb         17.7mb
yellow open   books               I8Zcx3VvR8aRrFDidOEqXg   1   1          0            0       283b           283b
yellow open   logstash-2015.05.20 aXM9nZz7TlOUaHUj9joMPA   1   1       4750            0     18.4mb         18.4mb
yellow open   logstash-2015.05.18 dshO3QMiQx2bnOgi7nbY5w   1   1       4631            0     18.3mb         18.3mb
yellow open   employee            Dxb9NFNjQVCtKrm00Wx7ag   1   1          0            0       283b           283b



#Cluster health 
[root@node1 bin]# ./curl -XGET localhost:9200/_cluster/health?pretty
{
  "cluster_name" : "elasticsearch",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 1,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 8,
  "active_shards" : 8,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 8,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 50.0
}


#image settings

/curl -XGET lo./curl -XGET localhost:9200/movie/_settings?pretty
{
  "movie" : {
    "settings" : {
      "index" : {
        "creation_date" : "1563888534782",
        "number_of_shards" : "1",
        "number_of_replicas" : "1",
        "uuid" : "4EZ738mQRseKhDx3lOgznQ",
        "version" : {
          "created" : "7020099"
        },
        "provided_name" : "movie"
      }
    }
  }	







#Create a index movie with mappings
[root@node1 opt]# curl -H "Content-Type: application/json" -XPUT 127.0.0.1:9200/movies -d '
{
"mappings" : {

             "properties" : {
                      "year" : { "type" : "date" }
                            }
                  }
            }
'
{"acknowledged":true,"shards_acknowledged":true,"index":"movie"}l






General syntax
curl -XPOST "http://[localhost]:9200/indexname/typename/optionalUniqueId" -d '{ "field" : "value" }'



#TO list the data in elastic
curl -H "Content-Type: application/json"  http://localhost:9200/movie/_search


#To List all the items in movie index
./curl http://localhost:9200/movie/_search?pretty


#To retrive only one specific document from index
[root@node1 bin]# ./curl http://localhost:9200/movies/_doc/1924/?pretty
{
  "_index" : "movies",
  "_type" : "_doc",
  "_id" : "1924",
  "_version" : 1,
  "_seq_no" : 4,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "id" : "1924",
    "title" : "Plan 9 from Outer Space",
    "year" : 1959,
    "genre" : [
      "Horror",
      "Sci-Fi"
    ]
  }
}




json bulk insert in Elastic

curl -H "Content-Type: application/json" -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @movies.json
[root@node1 es_data]# cat movies.json
{ "create" : { "_index" : "movies", "_id" : "135569" } }
{ "id": "135569", "title" : "Star Trek Beyond", "year":2016 , "genre":["Action", "Adventure", "Sci-Fi"] }
{ "create" : { "_index" : "movies", "_id" : "122886" } }
{ "id": "122886", "title" : "Star Wars: Episode VII - The Force Awakens", "year":2015 , "genre":["Action", "Adventure", "Fantasy", "Sci-Fi", "IMAX"] }
{ "create" : { "_index" : "movies","_id" : "109487" } }
{ "id": "109487", "title" : "Interstellar", "year":2014 , "genre":["Sci-Fi", "IMAX"] }
{ "create" : { "_index" : "movies","_id" : "58559" } }
{ "id": "58559", "title" : "Dark Knight, The", "year":2008 , "genre":["Action", "Crime", "Drama", "IMAX"] }
{ "create" : { "_index" : "movies","_id" : "1924" } }
{ "id": "1924", "title" : "Plan 9 from Outer Space", "year":1959 , "genre":["Horror", "Sci-Fi"] }
[root@node1 es_data]#



###Updating the document
updating the doucment that is alredy index using _version
when you update a new document a new document is created with an incremented_version
old version will mark for deletion 


#Updating the document 
[root@node1 bin]# ./curl http://localhost:9200/movies/_doc/1924/_update -d '
> {
>       "doc": {
>                "title": "Intersteller woo"
>             }
> }'
{"_index":"movies","_type":"_doc","_id":"1924","_version":2,"result":"updated","_shards":{"total  

Check the verion is update to 2 ,Now try to retrive the data to same iD

[root@node1 bin]# ./curl http://localhost:9200/movies/_doc/1924?pretty
{
  "_index" : "movies",
  "_type" : "_doc",
  "_id" : "1924",
  "_version" : 2,
  "_seq_no" : 5,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "id" : "1924",
    "title" : "Intersteller woo",
    "year" : 1959,
    "genre" : [
      "Horror",
      "Sci-Fi"
    ]
  }
}


Note: "Interstell has been moved to Interstell woo"


##Deleting the document in the elasticserach 
we need to user _DELETE

--Before deletion of the document
[root@node1 bin]# ./curl http://localhost:9200/movies/_doc/1924?pretty
{
  "_index" : "movies",
  "_type" : "_doc",
  "_id" : "1924",
  "_version" : 2,
  "_seq_no" : 5,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "id" : "1924",
    "title" : "Intersteller woo",
    "year" : 1959,
    "genre" : [
      "Horror",	
      "Sci-Fi"	
    ]
  }
}

--Deleting the document
[root@node1 bin]# ./curl -XDELETE 127.0.0.1:9200/movies/_doc/1924
{"_index":"movies","_type":"_doc","_id":"1924","_version":3,"result":"deleted","_shards":{"total":2,"successful":1,"failed":0},"_seq_no":6,"_primary_term":1}[root@node1 bin]#
[root@node1 bin]#
[root@node1 bin]#

--After deleting the document
[root@node1 bin]# ./curl http://localhost:9200/movies/_doc/1924?pretty
{
  "_index" : "movies",
  "_type" : "_doc",
  "_id" : "1924",
  "found" : false
}
[root@node1 bin]#



#Searching the data based on the keywork
bin]# ./curl http://localhost:9200/movies/_search?q=Dark


{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"skipped":0,"failed":0},"hits":{"total":{"value":1,"relation":"eq"},"max_score":1.5169398,"hits":[{"_index":"movies","_type":"_doc","_id":"58559","_score":1.5169398,"_source":{ "id": "58559", "title" : "Dark Knight, The", "year":2008 , "genre":["Action", "Crime", "Drama", "IMAX"] }



#Updating settings of index
[root@node1 bin]# ./curl -XPUT 127.0.0.1:9200/myindex/_settings -d '
{
    "index" : {
        "number_of_replicas" : 2
    }
}'

[root@node1 bin]# ./curl -XGET 127.0.0.1:9200/myindex/_settings?pretty
{
  "myindex" : {
    "settings" : {
      "index" : {
        "creation_date" : "1563965279536",
        "number_of_shards" : "1",
        "number_of_replicas" : "2",
        "uuid" : "Zg0Kbw1bT1aGf-duSN5JzA",
        "version" : {
          "created" : "7020099"
        },
        "provided_name" : "myindex"
      }
    }
  }
}
